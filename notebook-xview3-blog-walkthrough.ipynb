{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect dark vessel fishing activity in satellite imagery on Amazon SageMaker\n",
    "\n",
    "## Background\n",
    "\n",
    "In this notebook, we demonstrate how to train, evaluate, and deploy .\n",
    "\n",
    "Steps:\n",
    "* ~~Setup~~\n",
    "* Data preparation:\n",
    "    * ~~prepare labels csv dataframe ~~\n",
    "        * ~~merge train + val~~\n",
    "        * ~~create new val, new train, and mini train for hyperparameter exploration~~\n",
    "    * ~~create annotations~~ \n",
    "        * ~~training:full scene~~\n",
    "        *  ~~full scene validation set (no need to do for public leaderboard)~~\n",
    "    * ~~SM processing jobs:\n",
    "        * ~~convert public and validation set to hdf5 for faster inference (it’s about 10x faster dataloading w/ hdf5 input than geotif images)~~\n",
    "        * ~~[optional] chip scenes, demo on tiny.~~\n",
    "    * Modify dataset dict w/ path?, check file paths and upload dataset dict to S3.\n",
    "    * Modify dataset dict for scenes/files actually present on machine when `s3shardedbykey=True`\n",
    "* Train\n",
    "* Inference on leaderboard images & scoring\n",
    "    * Batch Transform for inference\n",
    "        * time speed to convert to hdf5 from tif for speedup?\n",
    "    * SM processing for evaluation\n",
    "* Visualize -> move this to separate notebook?, this would not be part of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ec2-user/SageMaker/daemon.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/ec2-user/SageMaker/daemon.json\n",
    "{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"path\": \"nvidia-container-runtime\",\n",
    "            \"runtimeArgs\": []\n",
    "        }\n",
    "    },\n",
    "    \"default-shm-size\": \"200G\",\n",
    "    \"data-root\": \"/home/ec2-user/SageMaker/docker\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl stop docker.service\n",
      "Warning: Stopping docker.service, but it can still be activated by:\n",
      "  docker.socket\n",
      "Redirecting to /bin/systemctl start docker.service\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo service docker stop\n",
    "mkdir -p /home/ec2-user/SageMaker/docker\n",
    "sudo rsync -aqxP /var/lib/docker/ /home/ec2-user/SageMaker/docker\n",
    "sudo mv /var/lib/docker /var/lib/docker.old\n",
    "sudo mv /home/ec2-user/SageMaker/daemon.json /etc/docker/\n",
    "sudo service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, Processor\n",
    "\n",
    "sys.path.append('tools/')\n",
    "from docker_utils import build_and_push_docker_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define execution role, S3 bucket in account, and SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = 'xview3-blog-sagemaker'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "account = sagemaker_session.account_id()\n",
    "tags =[{'Key': 'project', 'Value': 'xview3-blog'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TINY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation with SageMaker Processing\n",
    "In this section we create the following from the xView3 challenge dataset:\n",
    "1. a new `train` and `valid`, after merging the train and validation set provided by the challenge. \n",
    "2. Detectron2 compatible dataset dicts to be used for training. \n",
    "\n",
    "\n",
    "#### Merge & split data labels. \n",
    "The xView3 challenge provided detection labels for each scene in `train.csv`, `validation.csv`, and `public.csv`. \n",
    "We will merge the `train.csv` and `validation.csv` and create a new `train` and `validation` set for training. The `public` leaderboard set will remain fixed.\n",
    "\n",
    "#### Create Detectron2 Datasets\n",
    "Here we create the Detectron2-compatible [dataset dicts](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html) used for training models in Detectron2. The format of the dataset is a list of dictionaries with each dict containing information for one image with at least the following fields:\n",
    "- `filename`:str\n",
    "- `height`:int\n",
    "- `width`: int\n",
    "- `image_id`:str or int\n",
    "- `annotations`: list[dict]\n",
    "\n",
    "For more information on how to generate the dataset dict, see [Detectron2 docs] (https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#standard-dataset-dicts).\n",
    "\n",
    "Our dataset dict is generated from the information provided in the label `csv` files used in the previous section. Depending on whether we train our models with inputs originating from image chips (tiles) or from the full scene we will use one of two functions in the `xview3_d2` pacakage: `create_xview3_chipped_scene_annotations` or `create_xview3_full_scene_annotations`, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build base image for SM Processing tasks.\n",
    "For convenience, we build a base processing container which handles package installations. We can build a subsequent image from this base container to include the code we want to run. Here is what the base processing container looks like.\n",
    "\n",
    "For building and pushing the containers, we use helper function `build_and_push_docker_image` in `tools/docker_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mubuntu:20.04\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mDEBIAN_FRONTEND\u001b[39;49;00m=noninteractive\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update \u001b[33m\\\u001b[39;49;00m\n",
      "    && apt-get -y install python3 python3-pip vim nano git\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m requirements_cpu.txt .\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install -r requirements_cpu.txt\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m python3 -m pip install \u001b[33m'git+https://github.com/facebookresearch/detectron2.git'\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m src /opt/ml/code/\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install /opt/ml/code/src\n",
      "\n",
      "\u001b[37m# Make sure python doesn't buffer stdout so we get logs ASAP.\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=TRUE\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l docker docker/processing/base.Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build and push the base processing container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building docker image xview3-processing:base from docker/processing/base.Dockerfile\n",
      "$ docker build -t xview3-processing:base -f docker/processing/base.Dockerfile .\n",
      "Sending build context to Docker daemon  736.3kB\n",
      "Step 1/8 : FROM ubuntu:20.04\n",
      " ---> 20fffa419e3a\n",
      "Step 2/8 : ENV DEBIAN_FRONTEND=noninteractive\n",
      " ---> Using cache\n",
      " ---> 0654e794ab1a\n",
      "Step 3/8 : RUN apt-get update     && apt-get -y install python3 python3-pip vim nano git\n",
      " ---> Using cache\n",
      " ---> ccba904afcad\n",
      "Step 4/8 : COPY requirements_cpu.txt .\n",
      " ---> b25e8ef26586\n",
      "Step 5/8 : RUN pip3 install -r requirements_cpu.txt\n",
      " ---> Running in fe81aadb268a\n",
      "Collecting h5py==3.6.0\n",
      "  Downloading h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "Collecting iopath==0.1.9\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Collecting opencv_python_headless>=4.5\n",
      "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
      "Collecting pandas==1.3.3\n",
      "  Downloading pandas-1.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Collecting pycocotools==2.0.3\n",
      "  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n",
      "Collecting rasterio==1.2.10\n",
      "  Downloading rasterio-1.2.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (19.2 MB)\n",
      "Collecting scikit_learn==1.1.1\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "Collecting scipy==1.8.0\n",
      "  Downloading scipy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
      "Collecting tabulate==0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting torch==1.10.0\n",
      "  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "Collecting torchvision==0.11.1\n",
      "  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "Collecting tqdm==4.62.3\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting zarr==2.11.1\n",
      "  Downloading zarr-2.11.1-py3-none-any.whl (153 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Collecting cython>=0.27.3\n",
      "  Using cached Cython-0.29.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Collecting matplotlib>=2.1.0\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/lib/python3/dist-packages (from pycocotools==2.0.3->-r requirements_cpu.txt (line 6)) (45.2.0)\n",
      "Collecting affine\n",
      "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
      "Collecting certifi\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Collecting snuggs>=1.4.1\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting attrs\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting click-plugins\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting click>=4.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting numcodecs>=0.6.4\n",
      "  Downloading numcodecs-0.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Collecting fasteners\n",
      "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
      "Collecting asciitree\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Building wheels for collected packages: pycocotools, asciitree\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.3-cp38-cp38-linux_x86_64.whl size=418880 sha256=0a5e259066579bad8ad98aa1fc2fa0e4be8c2cad549f71b0cd67750045691db4\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/5b/26/04441bc1820bf3622e0ea8616bef01b02cad3415ad880b834a\n",
      "  Building wheel for asciitree (setup.py): started\n",
      "  Building wheel for asciitree (setup.py): finished with status 'done'\n",
      "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5035 sha256=a246325a2ebadb2ab6077b7c5836cb127915e6bffdd2a9e36ccd95e117d540d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/d7/75/19cd0d2a893cad4bb0b2b16dd572ad2916d19c0d5ee9612511\n",
      "Successfully built pycocotools asciitree\n",
      "Installing collected packages: numpy, h5py, portalocker, tqdm, iopath, opencv-python-headless, six, python-dateutil, pytz, pandas, cython, fonttools, pyparsing, kiwisolver, pillow, packaging, cycler, matplotlib, pycocotools, affine, certifi, click, cligj, snuggs, attrs, click-plugins, rasterio, joblib, scipy, threadpoolctl, scikit-learn, tabulate, typing-extensions, torch, torchvision, entrypoints, numcodecs, fasteners, asciitree, zarr\n",
      "Successfully installed affine-2.3.1 asciitree-0.3.3 attrs-22.1.0 certifi-2022.6.15 click-8.1.3 click-plugins-1.1.1 cligj-0.7.2 cycler-0.11.0 cython-0.29.32 entrypoints-0.4 fasteners-0.17.3 fonttools-4.34.4 h5py-3.6.0 iopath-0.1.9 joblib-1.1.0 kiwisolver-1.4.4 matplotlib-3.5.2 numcodecs-0.10.1 numpy-1.23.1 opencv-python-headless-4.6.0.66 packaging-21.3 pandas-1.3.3 pillow-9.2.0 portalocker-2.5.1 pycocotools-2.0.3 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.1 rasterio-1.2.10 scikit-learn-1.1.1 scipy-1.8.0 six-1.16.0 snuggs-1.4.7 tabulate-0.8.9 threadpoolctl-3.1.0 torch-1.10.0 torchvision-0.11.1 tqdm-4.62.3 typing-extensions-4.3.0 zarr-2.11.1\n",
      "Removing intermediate container fe81aadb268a\n",
      " ---> 400aecd4a282\n",
      "Step 6/8 : RUN python3 -m pip install detectron2 -f     https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n",
      " ---> Running in dbea8d0bdf96\n",
      "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html\n",
      "Collecting detectron2\n",
      "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/detectron2-0.6%2Bcpu-cp38-cp38-linux_x86_64.whl (5.4 MB)\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from detectron2) (0.8.9)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from detectron2) (2.0.3)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.8/dist-packages (from detectron2) (4.62.3)\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from detectron2) (0.1.9)\n",
      "Collecting omegaconf>=2.1\n",
      "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.8/dist-packages (from detectron2) (9.2.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting black==21.4b2\n",
      "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from detectron2) (3.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot->detectron2) (3.0.9)\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.8/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.32)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/lib/python3/dist-packages (from pycocotools>=2.0.2->detectron2) (45.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.23.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard->detectron2) (0.34.2)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.5.1)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from hydra-core>=1.1->detectron2) (21.3)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.9.0-py3-none-any.whl (33 kB)\n",
      "Collecting regex>=2020.1.8\n",
      "  Downloading regex-2022.7.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
      "Collecting pathspec<1,>=0.8.1\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting toml>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from black==21.4b2->detectron2) (8.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2022.6.15)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor, fvcore, future, antlr4-python3-runtime\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=03bd2fba0969ae2f57cbbcb37615d6a7a42b220c461fa3220e7fda5c274a0aa5\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=55123bbae8de2f22e61f1587f8958484c9edf2cf530b1b606d3eb5d8823176d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/f4/d9/8b3c3f254c28aa2daf5e2f5a8070b0a960278733fd2eb1f7a2\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=f74981693ee1d7fd9e1df071c7416a92d09e57fd6d1c76b13fcf7468be57f0e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=e570d39a7a86dbe4755b3c6ddbd0d6f63119896d4d8735d279cdb4d8906b4b9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "Successfully built termcolor fvcore future antlr4-python3-runtime\n",
      "Installing collected packages: pydot, PyYAML, yacs, cloudpickle, termcolor, fvcore, absl-py, tensorboard-data-server, zipp, importlib-metadata, markdown, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, MarkupSafe, werkzeug, idna, urllib3, charset-normalizer, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, grpcio, tensorboard, antlr4-python3-runtime, omegaconf, importlib-resources, hydra-core, future, regex, pathspec, mypy-extensions, toml, appdirs, black, detectron2\n",
      "Successfully installed MarkupSafe-2.1.1 PyYAML-6.0 absl-py-1.2.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 black-21.4b2 cachetools-5.2.0 charset-normalizer-2.1.0 cloudpickle-2.1.0 detectron2-0.6+cpu future-0.18.2 fvcore-0.1.5.post20220512 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 hydra-core-1.2.0 idna-3.3 importlib-metadata-4.12.0 importlib-resources-5.9.0 markdown-3.4.1 mypy-extensions-0.4.3 oauthlib-3.2.0 omegaconf-2.2.2 pathspec-0.9.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.2 regex-2022.7.25 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-1.1.0 toml-0.10.2 urllib3-1.26.11 werkzeug-2.2.1 yacs-0.1.8 zipp-3.8.1\n",
      "Removing intermediate container dbea8d0bdf96\n",
      " ---> 9c0fb82d5adc\n",
      "Step 7/8 : WORKDIR /opt/ml/code\n",
      " ---> Running in c7ed480eae39\n",
      "Removing intermediate container c7ed480eae39\n",
      " ---> 7a246dbace63\n",
      "Step 8/8 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in b14af1a9f266\n",
      "Removing intermediate container b14af1a9f266\n",
      " ---> fe12288332e8\n",
      "Successfully built fe12288332e8\n",
      "Successfully tagged xview3-processing:base\n",
      "Done building docker image xview3-processing:base\n",
      "ECR repository already exists: xview3-processing\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "$ docker tag xview3-processing:base 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:base\n",
      "Pushing docker image to ECR repository 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:base\n",
      "\n",
      "$ docker push 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:base\n",
      "The push refers to repository [869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing]\n",
      "85233d39b6a6: Preparing\n",
      "a15d68819e4a: Preparing\n",
      "b24dad29084b: Preparing\n",
      "e16db414196c: Preparing\n",
      "236e25fb2e53: Preparing\n",
      "af7ed92504ae: Preparing\n",
      "af7ed92504ae: Waiting\n",
      "236e25fb2e53: Layer already exists\n",
      "af7ed92504ae: Layer already exists\n",
      "e16db414196c: Pushed\n",
      "85233d39b6a6: Pushed\n",
      "a15d68819e4a: Pushed\n",
      "b24dad29084b: Pushed\n",
      "base: digest: sha256:1ed05171df0d7ef053ae76ecdfd1d91021f7d3f53ec36434dfbbd09c241ebbaa size: 1582\n",
      "Done pushing 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:base\n"
     ]
    }
   ],
   "source": [
    "processing_base_name = 'xview3-processing:base'\n",
    "base_image = build_and_push_docker_image(processing_base_name, \n",
    "                                         dockerfile='docker/processing/base.Dockerfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the main processing container, which copies the `.py` scripts in `tools/`. In each processing job, to follow, we can specify which the entrypoint `.py` script to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m BASE_IMAGE\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m${BASE_IMAGE}\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m src /opt/ml/code/src\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install /opt/ml/code/src\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m tools/ /opt/ml/code/\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l docker docker/processing/main.Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and push main processing container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_base_image = f'{account}.dkr.ecr.{region}.amazonaws.com/xview3-processing:base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building docker image xview3-processing:main from docker/processing/main.Dockerfile\n",
      "$ docker build -t xview3-processing:main -f docker/processing/main.Dockerfile . --build-arg BASE_IMAGE=869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:base\n",
      "Sending build context to Docker daemon  736.3kB\n",
      "Step 1/6 : ARG BASE_IMAGE\n",
      "Step 2/6 : FROM ${BASE_IMAGE}\n",
      " ---> fe12288332e8\n",
      "Step 3/6 : WORKDIR /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 6476897a5667\n",
      "Step 4/6 : COPY src /opt/ml/code/src\n",
      " ---> 305c10561b09\n",
      "Step 5/6 : RUN pip install /opt/ml/code/src\n",
      " ---> Running in 3f46a00d489f\n",
      "Processing ./src\n",
      "Building wheels for collected packages: xview3-d2\n",
      "  Building wheel for xview3-d2 (setup.py): started\n",
      "  Building wheel for xview3-d2 (setup.py): finished with status 'done'\n",
      "  Created wheel for xview3-d2: filename=xview3_d2-1.0-py3-none-any.whl size=52530 sha256=7919bf72c0be99974bfdf3dd02ea037ed65e9b238e4d2c09e4061a02dba90785\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0k0xse6n/wheels/4d/ae/7d/713a791d6e7bfdb5745a965d8c6dc4614611ae736c305153b8\n",
      "Successfully built xview3-d2\n",
      "Installing collected packages: xview3-d2\n",
      "Successfully installed xview3-d2-1.0\n",
      "Removing intermediate container 3f46a00d489f\n",
      " ---> 86564c608b97\n",
      "Step 6/6 : COPY tools/ /opt/ml/code/\n",
      " ---> a484b5088dd8\n",
      "Successfully built a484b5088dd8\n",
      "Successfully tagged xview3-processing:main\n",
      "Done building docker image xview3-processing:main\n",
      "ECR repository already exists: xview3-processing\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "$ docker tag xview3-processing:main 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:main\n",
      "Pushing docker image to ECR repository 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:main\n",
      "\n",
      "$ docker push 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:main\n",
      "The push refers to repository [869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing]\n",
      "6082fa7754de: Preparing\n",
      "dbd89176026c: Preparing\n",
      "c923e0936d1e: Preparing\n",
      "85233d39b6a6: Preparing\n",
      "a15d68819e4a: Preparing\n",
      "b24dad29084b: Preparing\n",
      "e16db414196c: Preparing\n",
      "236e25fb2e53: Preparing\n",
      "af7ed92504ae: Preparing\n",
      "e16db414196c: Waiting\n",
      "236e25fb2e53: Waiting\n",
      "af7ed92504ae: Waiting\n",
      "b24dad29084b: Waiting\n",
      "a15d68819e4a: Layer already exists\n",
      "85233d39b6a6: Layer already exists\n",
      "b24dad29084b: Layer already exists\n",
      "e16db414196c: Layer already exists\n",
      "236e25fb2e53: Layer already exists\n",
      "af7ed92504ae: Layer already exists\n",
      "c923e0936d1e: Pushed\n",
      "6082fa7754de: Pushed\n",
      "dbd89176026c: Pushed\n",
      "main: digest: sha256:76efa83de9b7c5ee0d14f2ca26ae38aee1a219dc73a47b3a71dd079c7054323e size: 2211\n",
      "Done pushing 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-processing:main\n"
     ]
    }
   ],
   "source": [
    "processing_main_name = 'xview3-processing:main'\n",
    "processing_main_image = build_and_push_docker_image(processing_main_name, \n",
    "                                                    dockerfile='docker/processing/main.Dockerfile', \n",
    "                                                    base_image=base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_main_image = f'{account}.dkr.ecr.{region}.amazonaws.com/xview3-processing:main'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch SageMaker Processing job for dataset preparation. \n",
    "The SageMaker Processing task will run `tools/create_xview3_dataset_dict.py`. This script creates a detectron2-compatible dataset dict for full scene imagery or chipped scenes. Optionally, this script will merge train and validation csvs and create a new split. \n",
    "\n",
    "Let's see the arguments required this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"Create detectron2 dataset dict for xView3.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mxview3_d2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mxview3\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    create_xview3_full_scene_annotations,\n",
      "    create_xview3_chipped_scene_annotations,\n",
      "    CHANNELS,\n",
      "    create_data_split,\n",
      ")\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mxview3_d2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m save_dataset, configure_logging\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Union, Tuple, List, Callable\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m partial\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\n",
      "logger = configure_logging(\u001b[33m\"\u001b[39;49;00m\u001b[33mCreate-xview3-dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Parse commandline arguments.\"\"\"\u001b[39;49;00m\n",
      "    parser = ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mSave xView3 imagery to File Storage format.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset-type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfull\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        choices=[\u001b[33m\"\u001b[39;49;00m\u001b[33mfull\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mchipped\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mType of xview3 dataset dictionary to create, full scenes or chipped scenes.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-labels-csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mGroundtruth train labels csv.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--valid-labels-csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mGroundtruth validation labels csv.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--tiny-labels-csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtiny\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mGroundtruth tiny subset labels csv.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--prop-valid-scenes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.85\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mProportion of validation scenes to include in training set. Default 0.85\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mRandom seed value. Default None, will generate a random seed.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--exclude-bay-of-biscay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mExclude scenes from Bay of Biscay from training set, which is not present in validation set. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/data/prepared/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDirectory to save split data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--gt-labels-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDirectory of groundtruth labels. Provide directory to skip data split.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--scene-stats-csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/stats/scene_stats.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mFull path to CSV of scene level statistics.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--bbox-width\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34m6\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mBounding box width to fill missing widths in training set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--bbox-height\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34m8\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mBounding box height to fill missing heights in training set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--data-channels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=[\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mVH_dB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mVV_dB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbathymetry\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ],\n",
      "        nargs=\u001b[33m\"\u001b[39;49;00m\u001b[33m+\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mData channels to include in image chip. Choices \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCHANNELS\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    \u001b[37m# Arguments for chipped scenes dataset\u001b[39;49;00m\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--chip-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m2560\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSize of image chip/tile.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--empyt-chip-overlap\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34m32\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOverlap (in pixels) between adjacent image chips.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--prop-keep-empty\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.2\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mProportion of empty chips to include in training set. Default 0.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--shoreline-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/shoreline/trainval/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDirectory to shoreline data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_xview3_dataset_dict\u001b[39;49;00m(\n",
      "    create_xview3_func: Callable,\n",
      "    gt_labels_csv: Union[\u001b[36mstr\u001b[39;49;00m, os.PathLike],\n",
      "    scene_stats_csv: Union[\u001b[36mstr\u001b[39;49;00m, os.PathLike],\n",
      "    output_dir: Union[\u001b[36mstr\u001b[39;49;00m, os.PathLike],\n",
      "    data_channels: List[\u001b[36mstr\u001b[39;49;00m],\n",
      "    bbox_width: \u001b[36mint\u001b[39;49;00m = \u001b[34m6\u001b[39;49;00m,\n",
      "    bbox_height: \u001b[36mint\u001b[39;49;00m = \u001b[34m8\u001b[39;49;00m,\n",
      "    name_prefix: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mxview3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    save_json: \u001b[36mbool\u001b[39;49;00m = \u001b[34mFalse\u001b[39;49;00m,\n",
      ") -> Tuple[Union[\u001b[36mstr\u001b[39;49;00m, os.PathLike], List[\u001b[36mdict\u001b[39;49;00m]]:\n",
      "    \u001b[33m\"\"\"Create Detectron2 dataset dict for all scenes.\u001b[39;49;00m\n",
      "\u001b[33m    Runs `create_xview3_full_scene_annotations` or `create_xview3_chipped_scene_annotations` in a loop for all scenes.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        create_xview3_func (Callable): callable function, either `create_xview3_full_scene_annotations`\u001b[39;49;00m\n",
      "\u001b[33m                or `create_xview3_chipped_scene_annotations`\u001b[39;49;00m\n",
      "\u001b[33m        gt_labels_csv (Union[str, os.PathLike]): groundtruth CSV of labels.\u001b[39;49;00m\n",
      "\u001b[33m        scene_stats_csv (Union[str, os.PathLike]): CSV of scene image statistics.\u001b[39;49;00m\n",
      "\u001b[33m        data_channels (List[str]): list of channels to include in dataset.\u001b[39;49;00m\n",
      "\u001b[33m        bbox_width (int): bounding box width to assign to detections if none present in annotations. default 6.\u001b[39;49;00m\n",
      "\u001b[33m        bbox_height (int): bounding box height to assign to detections if none present in annotations. default 6.\u001b[39;49;00m\n",
      "\u001b[33m        output_dir (Union[str, os.PathLike]): destination to save dataset.\u001b[39;49;00m\n",
      "\u001b[33m        name_prefix (str): prefix to add to dataset name upon saving.\u001b[39;49;00m\n",
      "\u001b[33m        save_json (bool): Save dataset dict as json. Default False, saves as pickle object.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\n",
      "\u001b[33m        dataset_path, dataset (Tuple[Union[str, os.PathLike], List[dict]]): path to saved dataset, dataset_dict\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    df_gt = pd.read_csv(gt_labels_csv)\n",
      "    df_stats = pd.read_csv(scene_stats_csv)\n",
      "\n",
      "    output_records = []\n",
      "    scenes = df_gt.scene_id.unique()\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCreating xView3 detectron2 dataset dict for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(scenes)\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m scene_id \u001b[35min\u001b[39;49;00m scenes:\n",
      "        df_scene_gt = df_gt[df_gt.scene_id == scene_id].reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        df_scene_stats = df_stats[df_stats.scene_id == scene_id].reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        records = create_xview3_func(\n",
      "            scene_id=scene_id,\n",
      "            df_scene_gt=df_scene_gt,\n",
      "            df_scene_stats=df_scene_stats,\n",
      "            channels=data_channels,\n",
      "            bbox_width=bbox_width,\n",
      "            bbox_height=bbox_height,\n",
      "        )\n",
      "        output_records.append(records)\n",
      "\n",
      "    Path(output_dir).mkdir(exist_ok=\u001b[34mTrue\u001b[39;49;00m, parents=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    output_fn = Path(output_dir) / \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mname_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mPath(gt_labels_csv).stem\u001b[33m}\u001b[39;49;00m\u001b[33m.dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    logger.info(\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m#scenes: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(scenes) \u001b[33m}\u001b[39;49;00m\u001b[33m, #images(items) in dataset: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(output_records)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    save_dataset(output_records, output_fn, save_as_json=save_json)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSaved dataset_dict to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mstr\u001b[39;49;00m(output_fn)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_fn, output_records\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    args = parse_args()\n",
      "    output_dir = Path(args.output_dir) / datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    output_dir.mkdir(exist_ok=\u001b[34mTrue\u001b[39;49;00m, parents=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    labels_dir = args.gt_labels_dir\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m labels_dir \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34massert\u001b[39;49;00m (\n",
      "            Path(args.train_labels_csv).is_file()\n",
      "            \u001b[35mand\u001b[39;49;00m Path(args.valid_labels_csv).is_file()\n",
      "        ), \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mVerify \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_labels_csv\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.valid_labels_csv\u001b[33m}\u001b[39;49;00m\u001b[33m files exist.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        logger.info(\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mMerging \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_labels_csv\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.valid_labels_csv\u001b[33m}\u001b[39;49;00m\u001b[33m creating new dataset split.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "        labels_dir = output_dir / \u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        create_data_split(\n",
      "            train_labels_csv=args.train_labels_csv,\n",
      "            valid_labels_csv=args.valid_labels_csv,\n",
      "            tiny_labels_csv=args.tiny_labels_csv,\n",
      "            output_dir=labels_dir,\n",
      "            prop_valid_scenes=args.prop_valid_scenes,\n",
      "            exclude_bay_of_biscay=args.exclude_bay_of_biscay,\n",
      "            seed=args.seed,\n",
      "        )\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.dataset_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mfull\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        create_dataset_func = (create_xview3_full_scene_annotations,)\n",
      "        prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33mxview3-full\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34melif\u001b[39;49;00m args.dataset_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mchipped\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        prefix = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mxview3-chipped_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.chip_size\u001b[33m}\u001b[39;49;00m\u001b[33mx\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.chip_size\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        create_dataset_func = partial(\n",
      "            create_xview3_chipped_scene_annotations,\n",
      "            chip_size=args.chip_size,\n",
      "            empty_chip_overlap=args.empty_chip_overlap,\n",
      "            prop_keep_empty=args.prop_keep_empty,\n",
      "            shoreline_dir=args.shoreline_dir,\n",
      "        )\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUnknown `dataset_type=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.dataset_type\u001b[33m}\u001b[39;49;00m\u001b[33m. Choices: `full`, `chipped`\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "\n",
      "    d2_datasets_output_dir = output_dir / \u001b[33m\"\u001b[39;49;00m\u001b[33mdetectron2_dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m csv \u001b[35min\u001b[39;49;00m Path(labels_dir).glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m*.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        create_xview3_dataset_dict(\n",
      "            create_xview3_func=create_dataset_func,\n",
      "            gt_labels_csv=csv,\n",
      "            scene_stats_csv=args.scene_stats_csv,\n",
      "            output_dir=d2_datasets_output_dir,\n",
      "            data_channels=args.data_channels,\n",
      "            name_prefix=prefix,\n",
      "        )\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l python tools/create_xview3_dataset_dict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize SM Processing job. \n",
    "We only need 1 instance for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.t3.xlarge'\n",
    "volume_size_in_gb = 30 \n",
    "instance_count = 1\n",
    "base_job_name = 'xview3-dataset-prep'\n",
    "                      \n",
    "dataset_processor = Processor(image_uri=processing_image_name,\n",
    "                              role=role,\n",
    "                              instance_count=instance_count,\n",
    "                              base_job_name=base_job_name,\n",
    "                              instance_type=instance_type, \n",
    "                              volume_size_in_gb=volume_size_in_gb, \n",
    "                              entrypoint=['python3', 'create_xview3_dataset_dict.py'],\n",
    "                              sagemaker_session=sagemaker_session, \n",
    "                              tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify inputs and run processing job. \n",
    "\n",
    "`tools/create_xview3_dataset_dict.py` has several defaults, which can be overridden by providing the relevant argument in the processor `arugments`.  The cell below will launch a processor job that creates a new data split and creates a dataset dict for full scenes. To create a dataset dict for chipped scenes, change `dataset-type` to `chipped` and provide additional inputs and/or arguments such as `--shoreline_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "override = False\n",
    "current_timestamp = '202207250702'\n",
    "SEED = 46998886\n",
    "\n",
    "if override:\n",
    "    current_timestamp = datetime.now().strftime(\"%Y%m%d%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  xview3-dataset-prep-2022-08-01-18-02-06-937\n",
      "Inputs:  [{'InputName': 'labels', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/xview3-dataset-prep-2022-08-01-18-02-06-937/input/labels', 'LocalPath': '/opt/ml/processing/input/labels', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'stats', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/xview3-dataset-prep-2022-08-01-18-02-06-937/input/stats/scene-stats.csv', 'LocalPath': '/opt/ml/processing/input/scene-stats', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'prepared-dataset', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://xview3-blog/data/processing/202207250702', 'LocalPath': '/opt/ml/processing/output/prepared/', 'S3UploadMode': 'EndOfJob'}}]\n",
      "................................\u001b[34mINFO:Create-xview3-dataset:Merging /opt/ml/processing/input/labels/train.csv and /opt/ml/processing/input/labels/validation.csv creating new dataset split.\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Num. scenes in training set: 554\u001b[0m\n",
      "\u001b[34mNum. scenes in validation set: 50\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Excluding Bay of Biscay scenes.\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Excluded 315 scenes from training set. Num. training scenes: 239\u001b[0m\n",
      "\u001b[34mDEBUG:xview3_d2.data.datasets.xview3:Num. scenes in split 1: 42\u001b[0m\n",
      "\u001b[34mNum. scenes in split 2: 8\u001b[0m\n",
      "\u001b[34mDEBUG:xview3_d2.data.datasets.xview3:Num. annotations in train: 54360, Num. scenes: 281\u001b[0m\n",
      "\u001b[34mNum. annotations in val: 3412, Num. scenes: 8\u001b[0m\n",
      "\u001b[34mDEBUG:xview3_d2.data.datasets.xview3:\u001b[0m\n",
      "\u001b[34mDEBUG:xview3_d2.data.datasets.xview3:#Num. annotations in tiny train: 1679, Num. scenes: 5\u001b[0m\n",
      "\u001b[34m#Num. annotations in tiny val: 370, Num. scenes: 1\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Saved new train (merged train+val) set to: /opt/ml/processing/output/prepared/labels/train.csv\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Saved new validation set to: /opt/ml/processing/output/prepared/labels/valid.csv\u001b[0m\n",
      "\u001b[34mINFO:xview3_d2.data.datasets.xview3:Merge train & validation.\u001b[0m\n",
      "\u001b[34mprop valid scenes added to train: 0.85\u001b[0m\n",
      "\u001b[34mrandom seed: 46998886\u001b[0m\n",
      "\u001b[34mExclude Bay of Biscay scenes in training set: True\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 5 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 5, #images(items) in dataset: 5\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-full-tiny-train.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 281 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 281, #images(items) in dataset: 281\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-full-train.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 1 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 1, #images(items) in dataset: 1\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-full-tiny-valid.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 8 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 8, #images(items) in dataset: 8\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-full-valid.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Elasped time: 4.1972 seconds.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_labels = ProcessingInput(source='data/labels/', \n",
    "                               destination='/opt/ml/processing/input/labels',\n",
    "                              input_name='labels')\n",
    "input_stats = ProcessingInput(source='data/scene-stats.csv', \n",
    "                              destination='/opt/ml/processing/input/scene-stats',\n",
    "                             input_name='stats')\n",
    "\n",
    "job_output = ProcessingOutput(source='/opt/ml/processing/output/prepared/',  \n",
    "                              destination=f's3://xview3-blog/data/processing/{current_timestamp}',\n",
    "                              output_name='prepared-dataset')\n",
    "\n",
    "dataset_processor.run(inputs=[input_labels, input_stats], \n",
    "              outputs=[job_output],\n",
    "              arguments=[\"--dataset-type\", \"full\", \n",
    "                         \"--train-labels-csv\", f\"{input_labels.destination}/train.csv\",\n",
    "                         \"--valid-labels-csv\", f\"{input_labels.destination}/validation.csv\",\n",
    "                         \"--tiny-labels-csv\", f\"{input_labels.destination}/tiny.csv\",\n",
    "                         \"--scene-stats-csv\", f\"{input_stats.destination}/scene-stats.csv\",\n",
    "                         \"--seed\", str(SEED), \n",
    "                         \"--output-dir\", job_output.source,\n",
    "              ],\n",
    "              wait=True,\n",
    "              logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Run processing job to created dataset dict for chipped scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_image_name = f'{account}.dkr.ecr.{region}.amazonaws.com/xview3-processing:main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.t3.xlarge'\n",
    "volume_size_in_gb = 30 \n",
    "instance_count = 1\n",
    "base_job_name = 'xview3-dataset-prep'\n",
    "                      \n",
    "dataset_processor = Processor(image_uri=processing_image_name,\n",
    "                              role=role,\n",
    "                              instance_count=instance_count,\n",
    "                              base_job_name=base_job_name,\n",
    "                              instance_type=instance_type, \n",
    "                              volume_size_in_gb=volume_size_in_gb, \n",
    "                              entrypoint=['python3', 'create_xview3_dataset_dict.py'],\n",
    "                              sagemaker_session=sagemaker_session, \n",
    "                              tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  xview3-dataset-prep-2022-08-04-18-23-04-146\n",
      "Inputs:  [{'InputName': 'trn-labels', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/processing/202207250702/labels/train.csv', 'LocalPath': '/opt/ml/processing/input/labels/train', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'tiny-labels', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/processing/202207250702/labels/tiny-train.csv', 'LocalPath': '/opt/ml/processing/input/labels/tiny', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'stats', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog-sagemaker/xview3-dataset-prep-2022-08-04-18-23-04-146/input/stats/scene-stats.csv', 'LocalPath': '/opt/ml/processing/input/scene-stats', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-4', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/shoreline/trainval/', 'LocalPath': '/opt/ml/processing/input/shoreline/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'prepared-dataset', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://xview3-blog/data/processing/202207250702', 'LocalPath': '/opt/ml/processing/output/prepared/', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...............................\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 5 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 5, #images(items) in dataset: 1907\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-chipped_2560x2560-tiny-train.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Creating xView3 detectron2 dataset dict for 281 scenes.\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:#scenes: 281, #images(items) in dataset: 66320\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Saved dataset_dict to /opt/ml/processing/output/prepared/detectron2_dataset/xview3-chipped_2560x2560-train.dataset\u001b[0m\n",
      "\u001b[34mINFO:Create-xview3-dataset:Elasped time: 303.5391 seconds.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_destination_uri = f's3://xview3-blog/data/processing/{current_timestamp}'\n",
    "\n",
    "input_stats = ProcessingInput(source='data/scene-stats.csv', \n",
    "                              destination='/opt/ml/processing/input/scene-stats',\n",
    "                              input_name='stats')\n",
    "input_label_trn = ProcessingInput(source=f'{s3_destination_uri}/labels/train.csv',\n",
    "                                  destination='/opt/ml/processing/input/labels/train',\n",
    "                                  input_name='trn-labels')\n",
    "input_labels_tiny = ProcessingInput(source=f'{s3_destination_uri}/labels/tiny-train.csv',\n",
    "                                    destination='/opt/ml/processing/input/labels/tiny',\n",
    "                                    input_name='tiny-labels')\n",
    "inputs_shoreline = ProcessingInput(source='s3://xview3-blog/data/shoreline/trainval/', \n",
    "                                  destination='/opt/ml/processing/input/shoreline/')\n",
    "\n",
    "job_output = ProcessingOutput(source='/opt/ml/processing/output/prepared/',  \n",
    "                              destination=s3_destination_uri,\n",
    "                              output_name='prepared-dataset')\n",
    "\n",
    "dataset_processor.run(inputs=[input_label_trn, input_labels_tiny, input_stats, inputs_shoreline], \n",
    "                      outputs=[job_output],\n",
    "                      arguments=[\"--dataset-type\", \"chipped\", \n",
    "                                 \"--scene-stats-csv\", f\"{input_stats.destination}/scene-stats.csv\",\n",
    "                                 \"--seed\", str(SEED), \n",
    "                                 \"--output-dir\", job_output.source, \n",
    "                                 \"--shoreline-dir\", inputs_shoreline.destination,\n",
    "                                 \"--gt-labels-dir\", str(Path(input_label_trn.destination).parent)],\n",
    "                      wait=True,\n",
    "                      logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagery Preparation with SageMaker Processing\n",
    "We use SageMaker Processing to prepare our imagery for training. \n",
    "The imagery data will be uploaded to the SageMaker session S3 bucket under `imagery`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Save native scene imagery in file storage/\n",
    "For dynamically sampling from full scene imagery, we observed that we can speed up training and evaluation by a factor of 10 if the scene imagery was stored in `hdf5` format, compared to loading the provided GeoTIFF (Geostationary Earth Orbit Tagged Image File Format) imagery data with `rasterio`. This is also useful during inference for evaluation.\n",
    "\n",
    "Let's kick of SageMaker Processsing job to convert imagery to `hdf5`. This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  xview3-storage-2022-07-26-16-53-55-910\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/raw', 'LocalPath': '/opt/ml/processing/input/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'imagery', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://xview3-blog/data/processing/202207250702/imagery', 'LocalPath': '/opt/ml/processing/output/imagery/', 'S3UploadMode': 'Continuous'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:12: E225 missing whitespace around operator\n",
      "18:30: E124 closing bracket does not match visual indentation\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.t3.xlarge'\n",
    "volume_size_in_gb = 300 \n",
    "instance_count = 75\n",
    "                      \n",
    "s3_uri_source = 's3://xview3-blog/data/raw'\n",
    "s3_uri_imagery = f'{s3_destination_uri}/imagery'\n",
    "\n",
    "storage_processor = Processor(image_uri=processing_image_name,\n",
    "                              role=role,\n",
    "                              instance_count=instance_count, \n",
    "                              base_job_name='xview3-storage',\n",
    "                              instance_type=instance_type, \n",
    "                              volume_size_in_gb=volume_size_in_gb,\n",
    "                              entrypoint=['python3', 'store_xview3_imagery.py'],\n",
    "                              sagemaker_session=sagemaker_session,\n",
    "                              tags=tags,)\n",
    "\n",
    "storage_processor.run(inputs=[ProcessingInput(source=s3_uri_source, \n",
    "                                              destination='/opt/ml/processing/input/',\n",
    "                                              s3_data_distribution_type='ShardedByS3Key')], \n",
    "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output/imagery/', \n",
    "                                                destination=s3_uri_imagery,\n",
    "                                                output_name='imagery',\n",
    "                                                s3_upload_mode=\"Continuous\")],\n",
    "                      arguments=[\"--store-format\", \"hdf5\"],\n",
    "                      wait=False,\n",
    "                      logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. [Optional] Image chipping \n",
    "If we decide to train with image chips, we can also use SageMaker Processing to generate image chips using the dataset dict created in the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_destination_uri = f's3://xview3-blog/data/processing/{current_timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://xview3-blog/data/processing/202207250702/imagery'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_uri_imagery = f'{s3_destination_uri}/imagery'\n",
    "s3_uri_imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_image_name = f'{account}.dkr.ecr.{region}.amazonaws.com/xview3-processing:main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri_destination_base = f\"{s3_uri_imagery}/chipped-scenes\"\n",
    "s3_uri_source_base = \"s3://xview3-blog/data/raw\"\n",
    "s3_uri_d2_datasets = f'{s3_destination_uri}/detectron2_dataset/'\n",
    "\n",
    "\n",
    "d2_dataset_fn = f\"xview3-chipped_2560x2560-{'tiny' if USE_TINY else 'train'}.dataset\"\n",
    "num_instances = 2 if USE_TINY else 50 \n",
    "s3_uri_imagery_source = f\"{s3_uri_source_base}/{'tiny' if USE_TINY else 'trainval'}\"\n",
    "s3_uri_destination = f\"{s3_uri_destination_base}/{'tiny' if USE_TINY else 'train'}\"\n",
    "\n",
    "# specify local input data for SageMaker Processing job.\n",
    "input_scenes = ProcessingInput(source=s3_uri_imagery_source, \n",
    "                               destination='/opt/ml/processing/input/scenes/', \n",
    "                               s3_data_distribution_type='ShardedByS3Key')\n",
    "\n",
    "input_d2_dataset = ProcessingInput(source=s3_uri_d2_datasets, \n",
    "                                   destination='/opt/ml/processing/input/datasets/',)\n",
    "                                                \n",
    "job_output = ProcessingOutput(source='/opt/ml/processing/output/', \n",
    "                              destination=s3_uri_destination, \n",
    "                              s3_upload_mode=\"Continuous\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need at least 32GB CPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  xview3-chip-scenes-train-2022-08-04-22-26-45-367\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/raw/trainval', 'LocalPath': '/opt/ml/processing/input/scenes/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://xview3-blog/data/processing/202207250702/detectron2_dataset/', 'LocalPath': '/opt/ml/processing/input/datasets/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://xview3-blog/data/processing/202207250702/imagery/chipped-scenes/train', 'LocalPath': '/opt/ml/processing/output/', 'S3UploadMode': 'Continuous'}}]\n"
     ]
    }
   ],
   "source": [
    "chip_processor = Processor(image_uri=processing_image_name,\n",
    "                           role=role,\n",
    "                           instance_count=num_instances, \n",
    "                           base_job_name=f\"xview3-chip-scenes-{'tiny' if USE_TINY else 'train'}\", \n",
    "                           instance_type='ml.t3.2xlarge',#'ml.r5.xlarge', \n",
    "                           volume_size_in_gb=1024, \n",
    "                           entrypoint=['python3', 'chip_scenes_from_annotations.py'],\n",
    "                           sagemaker_session=sagemaker_session, \n",
    "                           tags=tags)\n",
    "\n",
    "chip_processor.run(inputs=[input_scenes, input_d2_dataset], \n",
    "                   outputs=[job_output],\n",
    "                   arguments=['--scenes-input-dir', input_scenes.destination,\n",
    "                              '--d2-dataset', f\"{input_d2_dataset.destination}/{d2_dataset_fn}\",],\n",
    "                   wait=USE_TINY,\n",
    "                   logs=USE_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CHIPPED = False\n",
    "LOCAL = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_dockerfile = str(Path(\"docker/training/base.Dockerfile\").resolve())\n",
    "train_dockerfile = str(Path(\"docker/training/main.Dockerfile\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m REGION\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-training:1.9.1-gpu-py38-cu111-ubuntu20.04\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mauthor\u001b[39;49;00m=\u001b[33m\"kachio@amazon.com\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mDEBIAN_FRONTEND\u001b[39;49;00m=noninteractive\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update \u001b[33m\\\u001b[39;49;00m\n",
      "  && apt-get -y install python3 python3-pip git python3-setuptools \u001b[33m\\\u001b[39;49;00m\n",
      "  && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -U sagemaker\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -U --upgrade \u001b[31mtorch\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.10.0+cu102 \u001b[31mtorchvision\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.11.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -U --no-cache-dir detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -U \u001b[31mboto3\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.17.18 pandas rasterio zarr\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -U --no-cache-dir pycocotools~=\u001b[34m2\u001b[39;49;00m.0.0\n",
      "\n",
      "\u001b[37m# Make sure python doesn't buffer stdout so we get logs ASAP.\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=TRUE\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFORCE_CUDA\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m\n",
      "\u001b[37m# Build D2 only for Volta architecture - V100 chips (ml.p3 AWS instances)\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mTORCH_CUDA_ARCH_LIST\u001b[39;49;00m=\u001b[33m\"Volta\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set a fixed model cache directory. Detectron2 requirement\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l docker {base_train_dockerfile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Base Training Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "Building docker image xview3-training:base from /home/ec2-user/SageMaker/xview3-blog/docker/training/base.Dockerfile\n",
      "$ docker build -t xview3-training:base -f /home/ec2-user/SageMaker/xview3-blog/docker/training/base.Dockerfile . --build-arg BASE_IMAGE=869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "Sending build context to Docker daemon  965.6kB\n",
      "Step 1/13 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.1-gpu-py38-cu111-ubuntu20.04\n",
      "1.9.1-gpu-py38-cu111-ubuntu20.04: Pulling from pytorch-training\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "d5f48f468589: Pulling fs layer\n",
      "1600774dceb6: Pulling fs layer\n",
      "d97603d2ab53: Pulling fs layer\n",
      "679ab295ca40: Pulling fs layer\n",
      "48d246cb90c2: Pulling fs layer\n",
      "bb41250418ce: Pulling fs layer\n",
      "44b4c20eb03f: Pulling fs layer\n",
      "c44412e812a9: Pulling fs layer\n",
      "752f0917fc9a: Pulling fs layer\n",
      "8b9d79ac0c89: Pulling fs layer\n",
      "5f10c434c774: Pulling fs layer\n",
      "65eec9735048: Pulling fs layer\n",
      "0ee812e2e88d: Pulling fs layer\n",
      "acf60e6945af: Pulling fs layer\n",
      "16eb0aea9e61: Pulling fs layer\n",
      "da5fd6c1f628: Pulling fs layer\n",
      "827e867cf7eb: Pulling fs layer\n",
      "af687e0d8f4b: Pulling fs layer\n",
      "7d46995ca178: Pulling fs layer\n",
      "02ffc4d7858f: Pulling fs layer\n",
      "891e11bfaf7c: Pulling fs layer\n",
      "65eec9735048: Waiting\n",
      "4ce2afea21b4: Pulling fs layer\n",
      "752f0917fc9a: Waiting\n",
      "bb41250418ce: Waiting\n",
      "cacf24635f86: Pulling fs layer\n",
      "5f10c434c774: Waiting\n",
      "0ee812e2e88d: Waiting\n",
      "a10aef6a8810: Pulling fs layer\n",
      "f3d6bc49ff1d: Pulling fs layer\n",
      "44b4c20eb03f: Waiting\n",
      "8b9d79ac0c89: Waiting\n",
      "14fe3684b2d6: Pulling fs layer\n",
      "d06b60df4f4a: Pulling fs layer\n",
      "acf60e6945af: Waiting\n",
      "7d46995ca178: Waiting\n",
      "a9365761537a: Pulling fs layer\n",
      "827e867cf7eb: Waiting\n",
      "25bc86a7fc07: Pulling fs layer\n",
      "599e659d9931: Pulling fs layer\n",
      "af687e0d8f4b: Waiting\n",
      "4ce2afea21b4: Waiting\n",
      "d06b60df4f4a: Waiting\n",
      "16eb0aea9e61: Waiting\n",
      "a9365761537a: Waiting\n",
      "da5fd6c1f628: Waiting\n",
      "14fe3684b2d6: Waiting\n",
      "02ffc4d7858f: Waiting\n",
      "f3d6bc49ff1d: Waiting\n",
      "679ab295ca40: Waiting\n",
      "25bc86a7fc07: Waiting\n",
      "d97603d2ab53: Waiting\n",
      "48d246cb90c2: Waiting\n",
      "cacf24635f86: Waiting\n",
      "d0dc25c03551: Pulling fs layer\n",
      "599e659d9931: Waiting\n",
      "87c89cc1374a: Pulling fs layer\n",
      "059852468284: Pulling fs layer\n",
      "c5fa58df1632: Pulling fs layer\n",
      "d0dc25c03551: Waiting\n",
      "a47fffa1516a: Pulling fs layer\n",
      "87c89cc1374a: Waiting\n",
      "26df4f4fbfd8: Pulling fs layer\n",
      "a1c96fc0fac8: Pulling fs layer\n",
      "c5fa58df1632: Waiting\n",
      "a1c96fc0fac8: Waiting\n",
      "26df4f4fbfd8: Waiting\n",
      "a47fffa1516a: Waiting\n",
      "d5f48f468589: Verifying Checksum\n",
      "d5f48f468589: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "679ab295ca40: Download complete\n",
      "d97603d2ab53: Verifying Checksum\n",
      "d97603d2ab53: Download complete\n",
      "1600774dceb6: Verifying Checksum\n",
      "1600774dceb6: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "d5f48f468589: Pull complete\n",
      "1600774dceb6: Pull complete\n",
      "d97603d2ab53: Pull complete\n",
      "44b4c20eb03f: Verifying Checksum\n",
      "44b4c20eb03f: Download complete\n",
      "679ab295ca40: Pull complete\n",
      "c44412e812a9: Verifying Checksum\n",
      "c44412e812a9: Download complete\n",
      "bb41250418ce: Verifying Checksum\n",
      "bb41250418ce: Download complete\n",
      "8b9d79ac0c89: Verifying Checksum\n",
      "8b9d79ac0c89: Download complete\n",
      "5f10c434c774: Download complete\n",
      "752f0917fc9a: Verifying Checksum\n",
      "752f0917fc9a: Download complete\n",
      "0ee812e2e88d: Verifying Checksum\n",
      "0ee812e2e88d: Download complete\n",
      "acf60e6945af: Download complete\n",
      "16eb0aea9e61: Download complete\n",
      "da5fd6c1f628: Verifying Checksum\n",
      "da5fd6c1f628: Download complete\n",
      "65eec9735048: Verifying Checksum\n",
      "65eec9735048: Download complete\n",
      "af687e0d8f4b: Verifying Checksum\n",
      "af687e0d8f4b: Download complete\n",
      "7d46995ca178: Download complete\n",
      "02ffc4d7858f: Download complete\n",
      "891e11bfaf7c: Verifying Checksum\n",
      "891e11bfaf7c: Download complete\n",
      "4ce2afea21b4: Verifying Checksum\n",
      "4ce2afea21b4: Download complete\n",
      "cacf24635f86: Verifying Checksum\n",
      "cacf24635f86: Download complete\n",
      "a10aef6a8810: Verifying Checksum\n",
      "a10aef6a8810: Download complete\n",
      "f3d6bc49ff1d: Verifying Checksum\n",
      "f3d6bc49ff1d: Download complete\n",
      "14fe3684b2d6: Verifying Checksum\n",
      "14fe3684b2d6: Download complete\n",
      "d06b60df4f4a: Verifying Checksum\n",
      "d06b60df4f4a: Download complete\n",
      "a9365761537a: Verifying Checksum\n",
      "25bc86a7fc07: Verifying Checksum\n",
      "25bc86a7fc07: Download complete\n",
      "599e659d9931: Verifying Checksum\n",
      "599e659d9931: Download complete\n",
      "d0dc25c03551: Verifying Checksum\n",
      "d0dc25c03551: Download complete\n",
      "87c89cc1374a: Download complete\n",
      "059852468284: Verifying Checksum\n",
      "059852468284: Download complete\n",
      "c5fa58df1632: Verifying Checksum\n",
      "c5fa58df1632: Download complete\n",
      "a47fffa1516a: Verifying Checksum\n",
      "a47fffa1516a: Download complete\n",
      "26df4f4fbfd8: Verifying Checksum\n",
      "26df4f4fbfd8: Download complete\n",
      "a1c96fc0fac8: Verifying Checksum\n",
      "a1c96fc0fac8: Download complete\n",
      "827e867cf7eb: Verifying Checksum\n",
      "827e867cf7eb: Download complete\n",
      "48d246cb90c2: Verifying Checksum\n",
      "48d246cb90c2: Download complete\n",
      "48d246cb90c2: Pull complete\n",
      "bb41250418ce: Pull complete\n",
      "44b4c20eb03f: Pull complete\n",
      "c44412e812a9: Pull complete\n",
      "752f0917fc9a: Pull complete\n",
      "8b9d79ac0c89: Pull complete\n",
      "5f10c434c774: Pull complete\n",
      "65eec9735048: Pull complete\n",
      "0ee812e2e88d: Pull complete\n",
      "acf60e6945af: Pull complete\n",
      "16eb0aea9e61: Pull complete\n",
      "da5fd6c1f628: Pull complete\n",
      "827e867cf7eb: Pull complete\n",
      "af687e0d8f4b: Pull complete\n",
      "7d46995ca178: Pull complete\n",
      "02ffc4d7858f: Pull complete\n",
      "891e11bfaf7c: Pull complete\n",
      "4ce2afea21b4: Pull complete\n",
      "cacf24635f86: Pull complete\n",
      "a10aef6a8810: Pull complete\n",
      "f3d6bc49ff1d: Pull complete\n",
      "14fe3684b2d6: Pull complete\n",
      "d06b60df4f4a: Pull complete\n",
      "a9365761537a: Pull complete\n",
      "25bc86a7fc07: Pull complete\n",
      "599e659d9931: Pull complete\n",
      "d0dc25c03551: Pull complete\n",
      "87c89cc1374a: Pull complete\n",
      "059852468284: Pull complete\n",
      "c5fa58df1632: Pull complete\n",
      "a47fffa1516a: Pull complete\n",
      "26df4f4fbfd8: Pull complete\n",
      "a1c96fc0fac8: Pull complete\n",
      "Digest: sha256:006ff547d0b94c8ca43ed0ebb282ca720f9ca77aec15d6b6f471b1ce87bdaca2\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.1-gpu-py38-cu111-ubuntu20.04\n",
      " ---> 415a836a3cea\n",
      "Step 2/13 : LABEL author=\"kachio@amazon.com\"\n",
      " ---> Running in 3b9c9bf0e0a7\n",
      "Removing intermediate container 3b9c9bf0e0a7\n",
      " ---> a39ea09b0608\n",
      "Step 3/13 : ENV DEBIAN_FRONTEND=noninteractive\n",
      " ---> Running in b3e071db0bb7\n",
      "Removing intermediate container b3e071db0bb7\n",
      " ---> 77a6334b7388\n",
      "Step 4/13 : RUN apt-get update   && apt-get -y install python3 python3-pip git python3-setuptools   && rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 353c0ec0c246\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [885 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2044 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1525 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1165 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2496 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.2 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.5 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1411 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Fetched 23.1 MB in 5s (4836 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "python3 is already the newest version (3.8.2-0ubuntu2).\n",
      "python3 set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  python-pip-whl python3-pkg-resources python3-wheel\n",
      "Suggested packages:\n",
      "  gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email\n",
      "  git-gui gitk gitweb git-cvs git-mediawiki git-svn python-setuptools-doc\n",
      "The following NEW packages will be installed:\n",
      "  python-pip-whl python3-pip python3-pkg-resources python3-setuptools\n",
      "  python3-wheel\n",
      "The following packages will be upgraded:\n",
      "  git\n",
      "1 upgraded, 5 newly installed, 0 to remove and 37 not upgraded.\n",
      "Need to get 7076 kB of archives.\n",
      "After this operation, 5541 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-pkg-resources all 45.2.0-1 [130 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 git amd64 1:2.25.1-1ubuntu3.5 [4557 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.6 [1805 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-setuptools all 45.2.0-1 [330 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 python3-wheel all 0.34.2-1 [23.8 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.6 [231 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 7076 kB in 1s (7588 kB/s)\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "(Reading database ... 44696 files and directories currently installed.)\n",
      "Preparing to unpack .../0-python3-pkg-resources_45.2.0-1_all.deb ...\n",
      "Unpacking python3-pkg-resources (45.2.0-1) ...\n",
      "Preparing to unpack .../1-git_1%3a2.25.1-1ubuntu3.5_amd64.deb ...\n",
      "Unpacking git (1:2.25.1-1ubuntu3.5) over (1:2.25.1-1ubuntu3.4) ...\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "Preparing to unpack .../2-python-pip-whl_20.0.2-5ubuntu1.6_all.deb ...\n",
      "Unpacking python-pip-whl (20.0.2-5ubuntu1.6) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../3-python3-setuptools_45.2.0-1_all.deb ...\n",
      "Unpacking python3-setuptools (45.2.0-1) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../4-python3-wheel_0.34.2-1_all.deb ...\n",
      "Unpacking python3-wheel (0.34.2-1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../5-python3-pip_20.0.2-5ubuntu1.6_all.deb ...\n",
      "Unpacking python3-pip (20.0.2-5ubuntu1.6) ...\n",
      "Setting up python3-pkg-resources (45.2.0-1) ...\n",
      "Setting up python3-setuptools (45.2.0-1) ...\n",
      "Setting up python3-wheel (0.34.2-1) ...\n",
      "Setting up git (1:2.25.1-1ubuntu3.5) ...\n",
      "Setting up python-pip-whl (20.0.2-5ubuntu1.6) ...\n",
      "Setting up python3-pip (20.0.2-5ubuntu1.6) ...\n",
      "Removing intermediate container 353c0ec0c246\n",
      " ---> 5a12f2fe7f2e\n",
      "Step 5/13 : RUN pip install --upgrade pip\n",
      " ---> Running in a12e2396bad0\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.1-py3-none-any.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 57.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container a12e2396bad0\n",
      " ---> ed2f4de156b0\n",
      "Step 6/13 : RUN pip install -U sagemaker\n",
      " ---> Running in dd4dba204dd6\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.75.1)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.101.1.tar.gz (552 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 552.8/552.8 kB 33.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.4.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.24.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.22.3)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (21.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.2.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.9)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.0->boto3<2.0,>=1.20.21->sagemaker) (1.26.6)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.101.1-py2.py3-none-any.whl size=767567 sha256=968ee91dbe6449f53135ccff41a4c0207070691c8807dc58983e3661b045985a\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/76/2f/3e7e7317eaf6bae083ce932f5c28c0544a1bfe321924c344bd\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.75.1\n",
      "    Uninstalling sagemaker-2.75.1:\n",
      "      Successfully uninstalled sagemaker-2.75.1\n",
      "Successfully installed sagemaker-2.101.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container dd4dba204dd6\n",
      " ---> 016ecd4683d6\n",
      "Step 7/13 : RUN pip install -U --upgrade torch==1.10.0+cu102 torchvision==0.11.1+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
      " ---> Running in cfa0cee0fc34\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.0+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torch-1.10.0%2Bcu102-cp38-cp38-linux_x86_64.whl (881.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 881.9/881.9 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.11.1+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.11.1%2Bcu102-cp38-cp38-linux_x86_64.whl (23.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/23.2 MB 30.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.0+cu102) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision==0.11.1+cu102) (1.22.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision==0.11.1+cu102) (9.1.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.1\n",
      "    Uninstalling torch-1.9.1:\n",
      "      Successfully uninstalled torch-1.9.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.10.1+cu111\n",
      "    Uninstalling torchvision-0.10.1+cu111:\n",
      "      Successfully uninstalled torchvision-0.10.1+cu111\n",
      "Successfully installed torch-1.10.0+cu102 torchvision-0.11.1+cu102\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container cfa0cee0fc34\n",
      " ---> fe6d365d5438\n",
      "Step 8/13 : RUN pip install -U --no-cache-dir detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n",
      " ---> Running in 57349d22f8e1\n",
      "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n",
      "Collecting detectron2\n",
      "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/detectron2-0.6%2Bcu102-cp38-cp38-linux_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.8/site-packages (from detectron2) (2.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from detectron2) (3.4.3)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 158.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from detectron2) (0.8.9)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from detectron2) (0.18.2)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 157.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting omegaconf>=2.1\n",
      "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.1/79.1 kB 181.0 MB/s eta 0:00:00\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.8/site-packages (from detectron2) (4.61.2)\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.8/site-packages (from detectron2) (9.1.1)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.1/151.1 kB 195.1 MB/s eta 0:00:00\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.6/106.6 kB 189.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting black==21.4b2\n",
      "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.0/131.0 kB 198.3 MB/s eta 0:00:00\n",
      "Collecting toml>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.8/site-packages (from black==21.4b2->detectron2) (8.1.3)\n",
      "Collecting regex>=2020.1.8\n",
      "  Downloading regex-2022.7.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 768.2/768.2 kB 219.7 MB/s eta 0:00:00\n",
      "Collecting pathspec<1,>=0.8.1\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.22.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.4.1)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 181.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.9.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2) (21.2)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2) (0.11.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2) (52.0.0.post20210125)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 181.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 211.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 202.4 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 196.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2) (0.36.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.8/167.8 kB 202.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2) (2.0.3)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 208.9 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 189.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2) (2.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (1.16.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 187.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.5/151.5 kB 211.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fvcore, antlr4-python3-runtime, pycocotools, termcolor\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=d1ce7a6c6a93c697114d0e88ede127e6b0d23a5fe47d197393dd8b4d84472f31\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lw5qnz_/wheels/bc/f4/d9/8b3c3f254c28aa2daf5e2f5a8070b0a960278733fd2eb1f7a2\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=a4fe661266d4d5f500b7222f19f0b138ccddb51cb010e785779fc9843669073a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lw5qnz_/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for pycocotools (pyproject.toml): started\n",
      "  Building wheel for pycocotools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp38-cp38-linux_x86_64.whl size=422838 sha256=873417f34fe1548af3469d5d212284cd82e55a9112b03e759aef8013979c74d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lw5qnz_/wheels/dd/e2/43/3e93cd653b3346b3d702bb0509bc611189f95d60407bff1484\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=8f1fad10c1c053ee88f43f9678e86baed6407c590cc69d44bfd1283a97d4bbe1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lw5qnz_/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built fvcore antlr4-python3-runtime pycocotools termcolor\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, mypy-extensions, appdirs, antlr4-python3-runtime, yacs, toml, tensorboard-data-server, regex, pydot, pyasn1-modules, protobuf, portalocker, pathspec, omegaconf, oauthlib, importlib-resources, grpcio, cachetools, absl-py, requests-oauthlib, markdown, iopath, hydra-core, google-auth, black, pycocotools, google-auth-oauthlib, fvcore, tensorboard, detectron2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "Successfully installed absl-py-1.2.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 black-21.4b2 cachetools-5.2.0 detectron2-0.6+cu102 fvcore-0.1.5.post20220512 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 hydra-core-1.2.0 importlib-resources-5.9.0 iopath-0.1.9 markdown-3.4.1 mypy-extensions-0.4.3 oauthlib-3.2.0 omegaconf-2.2.2 pathspec-0.9.0 portalocker-2.5.1 protobuf-3.19.4 pyasn1-modules-0.2.8 pycocotools-2.0.4 pydot-1.4.2 regex-2022.7.25 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-1.1.0 toml-0.10.2 yacs-0.1.8\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 57349d22f8e1\n",
      " ---> 99a4b11f6c8a\n",
      "Step 9/13 : RUN pip install -U boto3==1.17.18 pandas rasterio zarr\n",
      " ---> Running in 23cbf0d87523\n",
      "Collecting boto3==1.17.18\n",
      "  Downloading boto3-1.17.18-py2.py3-none-any.whl (130 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.3/130.3 kB 18.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.2.4)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 88.0 MB/s eta 0:00:00\n",
      "Collecting rasterio\n",
      "  Downloading rasterio-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/20.7 MB 67.9 MB/s eta 0:00:00\n",
      "Collecting zarr\n",
      "  Downloading zarr-2.12.0-py3-none-any.whl (185 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.8/185.8 kB 37.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3==1.17.18) (0.10.0)\n",
      "Collecting botocore<1.21.0,>=1.20.18\n",
      "  Downloading botocore-1.20.112-py2.py3-none-any.whl (7.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 83.1 MB/s eta 0:00:00\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 17.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.8/site-packages (from rasterio) (8.1.3)\n",
      "Collecting snuggs>=1.4.1\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting click-plugins\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from rasterio) (52.0.0.post20210125)\n",
      "Collecting affine\n",
      "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from rasterio) (21.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from rasterio) (2022.5.18.1)\n",
      "Collecting asciitree\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fasteners\n",
      "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
      "Collecting numcodecs>=0.6.4\n",
      "  Downloading numcodecs-0.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 91.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.18->boto3==1.17.18) (1.26.6)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from numcodecs>=0.6.4->zarr) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.8/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
      "Building wheels for collected packages: asciitree\n",
      "  Building wheel for asciitree (setup.py): started\n",
      "  Building wheel for asciitree (setup.py): finished with status 'done'\n",
      "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5036 sha256=8b37f0935b37770d06d8967458d40f2f3afd46e5a8df842d306bb0b5bc91f189\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/d7/75/19cd0d2a893cad4bb0b2b16dd572ad2916d19c0d5ee9612511\n",
      "Successfully built asciitree\n",
      "Installing collected packages: asciitree, affine, snuggs, fasteners, entrypoints, cligj, click-plugins, rasterio, pandas, numcodecs, botocore, zarr, s3transfer, boto3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.1\n",
      "    Uninstalling botocore-1.27.1:\n",
      "      Successfully uninstalled botocore-1.27.1\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.6.0\n",
      "    Uninstalling s3transfer-0.6.0:\n",
      "      Successfully uninstalled s3transfer-0.6.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.0\n",
      "    Uninstalling boto3-1.24.0:\n",
      "      Successfully uninstalled boto3-1.24.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.101.1 requires boto3<2.0,>=1.20.21, but you have boto3 1.17.18 which is incompatible.\n",
      "awscli 1.25.1 requires botocore==1.27.1, but you have botocore 1.20.112 which is incompatible.\n",
      "awscli 1.25.1 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.3.7 which is incompatible.\n",
      "\u001b[0mSuccessfully installed affine-2.3.1 asciitree-0.3.3 boto3-1.17.18 botocore-1.20.112 click-plugins-1.1.1 cligj-0.7.2 entrypoints-0.4 fasteners-0.17.3 numcodecs-0.10.2 pandas-1.4.3 rasterio-1.3.0 s3transfer-0.3.7 snuggs-1.4.7 zarr-2.12.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 23cbf0d87523\n",
      " ---> 1ba9431d13ab\n",
      "Step 10/13 : RUN pip install -U --no-cache-dir pycocotools~=2.0.0\n",
      " ---> Running in ca05b9fa8790\n",
      "Requirement already satisfied: pycocotools~=2.0.0 in /opt/conda/lib/python3.8/site-packages (2.0.4)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from pycocotools~=2.0.0) (3.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from pycocotools~=2.0.0) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.0) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.0) (9.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.0) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools~=2.0.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools~=2.0.0) (1.16.0)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container ca05b9fa8790\n",
      " ---> eed83a3876c1\n",
      "Step 11/13 : ENV FORCE_CUDA=\"1\"\n",
      " ---> Running in 549994ace5ea\n",
      "Removing intermediate container 549994ace5ea\n",
      " ---> 68491349e13b\n",
      "Step 12/13 : ENV TORCH_CUDA_ARCH_LIST=\"Volta\"\n",
      " ---> Running in 254d92507d81\n",
      "Removing intermediate container 254d92507d81\n",
      " ---> f8e1a5c96175\n",
      "Step 13/13 : ENV FVCORE_CACHE=\"/tmp\"\n",
      " ---> Running in acccc321aa20\n",
      "Removing intermediate container acccc321aa20\n",
      " ---> 6005678c93a5\n",
      "[Warning] One or more build-args [BASE_IMAGE] were not consumed\n",
      "Successfully built 6005678c93a5\n",
      "Successfully tagged xview3-training:base\n",
      "Done building docker image xview3-training:base\n",
      "ECR repository already exists: xview3-training\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "$ docker tag xview3-training:base 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "Pushing docker image to ECR repository 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "\n",
      "$ docker push 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "The push refers to repository [869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training]\n",
      "2bec2fd5c215: Preparing\n",
      "3534c2ae025e: Preparing\n",
      "135516ebbd77: Preparing\n",
      "6b8f1255faed: Preparing\n",
      "088f0f6c3d73: Preparing\n",
      "a2d839c63b4f: Preparing\n",
      "f1a50167c61e: Preparing\n",
      "db5abdf1ca8e: Preparing\n",
      "36febfb24590: Preparing\n",
      "efc3379ca486: Preparing\n",
      "f964a6d1bdf4: Preparing\n",
      "ac196fc8aded: Preparing\n",
      "40c691851aa3: Preparing\n",
      "b547475759de: Preparing\n",
      "4772fa125f82: Preparing\n",
      "d1a7a58f91b9: Preparing\n",
      "2acdee852ef3: Preparing\n",
      "f6ad8b3b20da: Preparing\n",
      "ce71aba18ddd: Preparing\n",
      "edf4033d0095: Preparing\n",
      "ec7289967c61: Preparing\n",
      "1ac28deff3c9: Preparing\n",
      "d2e3bf405be8: Preparing\n",
      "8151b6d8bf06: Preparing\n",
      "f964a6d1bdf4: Waiting\n",
      "9290a9cc6a8e: Preparing\n",
      "a2d839c63b4f: Waiting\n",
      "11d386c39bde: Preparing\n",
      "f1a50167c61e: Waiting\n",
      "18efc6c265a6: Preparing\n",
      "4772fa125f82: Waiting\n",
      "05bee8393555: Preparing\n",
      "ac196fc8aded: Waiting\n",
      "7a6612c0bb83: Preparing\n",
      "885ff7f3caf7: Preparing\n",
      "757f009697c5: Preparing\n",
      "d1a7a58f91b9: Waiting\n",
      "94e4a30378e5: Preparing\n",
      "68b9b6eb9fad: Preparing\n",
      "40c691851aa3: Waiting\n",
      "ce71aba18ddd: Waiting\n",
      "36febfb24590: Waiting\n",
      "17989ebae4aa: Preparing\n",
      "db5abdf1ca8e: Waiting\n",
      "b4fb898f5841: Preparing\n",
      "b547475759de: Waiting\n",
      "edf4033d0095: Waiting\n",
      "6e68adc289b2: Preparing\n",
      "2acdee852ef3: Waiting\n",
      "8151b6d8bf06: Waiting\n",
      "8144b9447a97: Preparing\n",
      "d2e3bf405be8: Waiting\n",
      "e94bd04067f0: Preparing\n",
      "efc3379ca486: Waiting\n",
      "1ac28deff3c9: Waiting\n",
      "662144a3106f: Preparing\n",
      "9290a9cc6a8e: Waiting\n",
      "ec7289967c61: Waiting\n",
      "18efc6c265a6: Waiting\n",
      "3967b64f52c8: Preparing\n",
      "1dcb04b911f2: Preparing\n",
      "7a6612c0bb83: Waiting\n",
      "11d386c39bde: Waiting\n",
      "05bee8393555: Waiting\n",
      "314288c02398: Preparing\n",
      "6e68adc289b2: Waiting\n",
      "f6ad8b3b20da: Waiting\n",
      "68b9b6eb9fad: Waiting\n",
      "36043ad8bfd8: Preparing\n",
      "b4fb898f5841: Waiting\n",
      "41f9128a0929: Preparing\n",
      "94e4a30378e5: Waiting\n",
      "885ff7f3caf7: Waiting\n",
      "bf8cedc62fb3: Preparing\n",
      "17989ebae4aa: Waiting\n",
      "757f009697c5: Waiting\n",
      "8144b9447a97: Waiting\n",
      "662144a3106f: Waiting\n",
      "3967b64f52c8: Waiting\n",
      "314288c02398: Waiting\n",
      "41f9128a0929: Waiting\n",
      "36043ad8bfd8: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "2bec2fd5c215: Pushed\n",
      "088f0f6c3d73: Pushed\n",
      "a2d839c63b4f: Pushed\n",
      "db5abdf1ca8e: Layer already exists\n",
      "36febfb24590: Layer already exists\n",
      "efc3379ca486: Layer already exists\n",
      "f964a6d1bdf4: Layer already exists\n",
      "ac196fc8aded: Layer already exists\n",
      "40c691851aa3: Layer already exists\n",
      "b547475759de: Layer already exists\n",
      "4772fa125f82: Layer already exists\n",
      "d1a7a58f91b9: Layer already exists\n",
      "2acdee852ef3: Layer already exists\n",
      "f6ad8b3b20da: Layer already exists\n",
      "ce71aba18ddd: Layer already exists\n",
      "edf4033d0095: Layer already exists\n",
      "ec7289967c61: Layer already exists\n",
      "1ac28deff3c9: Layer already exists\n",
      "d2e3bf405be8: Layer already exists\n",
      "8151b6d8bf06: Layer already exists\n",
      "9290a9cc6a8e: Layer already exists\n",
      "11d386c39bde: Layer already exists\n",
      "18efc6c265a6: Layer already exists\n",
      "05bee8393555: Layer already exists\n",
      "7a6612c0bb83: Layer already exists\n",
      "885ff7f3caf7: Layer already exists\n",
      "757f009697c5: Layer already exists\n",
      "94e4a30378e5: Layer already exists\n",
      "68b9b6eb9fad: Layer already exists\n",
      "17989ebae4aa: Layer already exists\n",
      "b4fb898f5841: Layer already exists\n",
      "6e68adc289b2: Layer already exists\n",
      "8144b9447a97: Layer already exists\n",
      "e94bd04067f0: Layer already exists\n",
      "662144a3106f: Layer already exists\n",
      "3967b64f52c8: Layer already exists\n",
      "1dcb04b911f2: Layer already exists\n",
      "314288c02398: Layer already exists\n",
      "36043ad8bfd8: Layer already exists\n",
      "41f9128a0929: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "f1a50167c61e: Pushed\n",
      "135516ebbd77: Pushed\n",
      "3534c2ae025e: Pushed\n",
      "6b8f1255faed: Pushed\n",
      "base: digest: sha256:70723d44fc844d540ab07ac389cddd909e960d25aa467abca5e1ae54c1192d77 size: 9791\n",
      "Done pushing 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "Base image: 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n"
     ]
    }
   ],
   "source": [
    "training_base_name = 'xview3-training:base'\n",
    "\n",
    "base_image_uri = build_and_push_docker_image(training_base_name,  \n",
    "                                             dockerfile=str(base_train_dockerfile),)\n",
    "print(f'Base image: {base_image_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m\u001b[37m \u001b[39;49;00mBASE_IMAGE\n",
      "\u001b[34mFROM\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[33m${BASE_IMAGE}\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[33m/opt/ml/code\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m\u001b[37m \u001b[39;49;00mtools/xview3_train_net.py /opt/ml/code/\n",
      "\u001b[34mCOPY\u001b[39;49;00m\u001b[37m \u001b[39;49;00mconfigs/xview3 /opt/ml/code/configs\n",
      "\u001b[34mCOPY\u001b[39;49;00m\u001b[37m \u001b[39;49;00msrc /opt/ml/code/src\n",
      "\u001b[34mRUN\u001b[39;49;00m\u001b[37m \u001b[39;49;00mpip install /opt/ml/code/src\n",
      "\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[37m \u001b[39;49;00mSAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[37m \u001b[39;49;00mSAGEMAKER_PROGRAM xview3_train_net.py\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l docker {train_dockerfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_base_name = 'xview3-training:base'\n",
    "base_image_uri = f'{account}.dkr.ecr.{region}.amazonaws.com/{training_base_name}'\n",
    "training_main_name = 'xview3-training:train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building docker image xview3-training:train from /home/ec2-user/SageMaker/xview3-blog/docker/training/main.Dockerfile\n",
      "$ docker build -t xview3-training:train -f /home/ec2-user/SageMaker/xview3-blog/docker/training/main.Dockerfile . --build-arg BASE_IMAGE=869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:base\n",
      "Sending build context to Docker daemon  1.013MB\n",
      "Step 1/11 : ARG BASE_IMAGE\n",
      "Step 2/11 : FROM ${BASE_IMAGE}\n",
      " ---> 6005678c93a5\n",
      "Step 3/11 : WORKDIR /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> de1eaaf98385\n",
      "Step 4/11 : COPY tools/train_net.py /opt/ml/code/\n",
      " ---> Using cache\n",
      " ---> a0479f110d75\n",
      "Step 5/11 : COPY configs/xview3 /opt/ml/code/configs\n",
      " ---> e9adeed1503b\n",
      "Step 6/11 : COPY src /opt/ml/code/src\n",
      " ---> b0c9e24c9558\n",
      "Step 7/11 : RUN python3 -m pip install /opt/ml/code/src\n",
      " ---> Running in b94b455d8649\n",
      "Processing ./src\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: xview3-d2\n",
      "  Building wheel for xview3-d2 (setup.py): started\n",
      "  Building wheel for xview3-d2 (setup.py): finished with status 'done'\n",
      "  Created wheel for xview3-d2: filename=xview3_d2-1.0-py3-none-any.whl size=53025 sha256=5ce9d59e25139dbc826ef4dd38f7a18c7ad08f76d3f37dafb3dbadd94f1ba0fe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0_ctrilr/wheels/4d/ae/7d/713a791d6e7bfdb5745a965d8c6dc4614611ae736c305153b8\n",
      "Successfully built xview3-d2\n",
      "Installing collected packages: xview3-d2\n",
      "Successfully installed xview3-d2-1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b94b455d8649\n",
      " ---> b08d9907b90d\n",
      "Step 8/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in 42bcd7c109ad\n",
      "Removing intermediate container 42bcd7c109ad\n",
      " ---> 56126f2941ef\n",
      "Step 9/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in bddcb2eaea6c\n",
      "Removing intermediate container bddcb2eaea6c\n",
      " ---> e78824158503\n",
      "Step 10/11 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in e221072bf550\n",
      "Removing intermediate container e221072bf550\n",
      " ---> d3028bd4556e\n",
      "Step 11/11 : ENV SAGEMAKER_PROGRAM train_net.py\n",
      " ---> Running in f79b15fc45b0\n",
      "Removing intermediate container f79b15fc45b0\n",
      " ---> 5ebfd0856b84\n",
      "Successfully built 5ebfd0856b84\n",
      "Successfully tagged xview3-training:train\n",
      "Done building docker image xview3-training:train\n",
      "ECR repository already exists: xview3-training\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Logged into ECR\n",
      "$ docker tag xview3-training:train 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:train\n",
      "Pushing docker image to ECR repository 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:train\n",
      "\n",
      "$ docker push 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:train\n",
      "The push refers to repository [869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training]\n",
      "a67d17b18bae: Preparing\n",
      "7a21321c0b24: Preparing\n",
      "18041e4ed604: Preparing\n",
      "03ef946f32ed: Preparing\n",
      "15314207c318: Preparing\n",
      "2bec2fd5c215: Preparing\n",
      "3534c2ae025e: Preparing\n",
      "135516ebbd77: Preparing\n",
      "6b8f1255faed: Preparing\n",
      "088f0f6c3d73: Preparing\n",
      "a2d839c63b4f: Preparing\n",
      "f1a50167c61e: Preparing\n",
      "db5abdf1ca8e: Preparing\n",
      "36febfb24590: Preparing\n",
      "efc3379ca486: Preparing\n",
      "f964a6d1bdf4: Preparing\n",
      "ac196fc8aded: Preparing\n",
      "40c691851aa3: Preparing\n",
      "b547475759de: Preparing\n",
      "4772fa125f82: Preparing\n",
      "d1a7a58f91b9: Preparing\n",
      "2acdee852ef3: Preparing\n",
      "f6ad8b3b20da: Preparing\n",
      "ce71aba18ddd: Preparing\n",
      "edf4033d0095: Preparing\n",
      "ec7289967c61: Preparing\n",
      "1ac28deff3c9: Preparing\n",
      "d2e3bf405be8: Preparing\n",
      "8151b6d8bf06: Preparing\n",
      "9290a9cc6a8e: Preparing\n",
      "11d386c39bde: Preparing\n",
      "18efc6c265a6: Preparing\n",
      "05bee8393555: Preparing\n",
      "7a6612c0bb83: Preparing\n",
      "885ff7f3caf7: Preparing\n",
      "757f009697c5: Preparing\n",
      "94e4a30378e5: Preparing\n",
      "68b9b6eb9fad: Preparing\n",
      "17989ebae4aa: Preparing\n",
      "b4fb898f5841: Preparing\n",
      "6e68adc289b2: Preparing\n",
      "8144b9447a97: Preparing\n",
      "e94bd04067f0: Preparing\n",
      "662144a3106f: Preparing\n",
      "3967b64f52c8: Preparing\n",
      "1dcb04b911f2: Preparing\n",
      "314288c02398: Preparing\n",
      "36043ad8bfd8: Preparing\n",
      "41f9128a0929: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "f1a50167c61e: Waiting\n",
      "db5abdf1ca8e: Waiting\n",
      "36febfb24590: Waiting\n",
      "efc3379ca486: Waiting\n",
      "f964a6d1bdf4: Waiting\n",
      "ac196fc8aded: Waiting\n",
      "40c691851aa3: Waiting\n",
      "b547475759de: Waiting\n",
      "4772fa125f82: Waiting\n",
      "d1a7a58f91b9: Waiting\n",
      "2acdee852ef3: Waiting\n",
      "f6ad8b3b20da: Waiting\n",
      "ce71aba18ddd: Waiting\n",
      "edf4033d0095: Waiting\n",
      "ec7289967c61: Waiting\n",
      "1ac28deff3c9: Waiting\n",
      "d2e3bf405be8: Waiting\n",
      "8151b6d8bf06: Waiting\n",
      "9290a9cc6a8e: Waiting\n",
      "11d386c39bde: Waiting\n",
      "18efc6c265a6: Waiting\n",
      "05bee8393555: Waiting\n",
      "7a6612c0bb83: Waiting\n",
      "885ff7f3caf7: Waiting\n",
      "757f009697c5: Waiting\n",
      "94e4a30378e5: Waiting\n",
      "68b9b6eb9fad: Waiting\n",
      "17989ebae4aa: Waiting\n",
      "b4fb898f5841: Waiting\n",
      "6e68adc289b2: Waiting\n",
      "8144b9447a97: Waiting\n",
      "e94bd04067f0: Waiting\n",
      "662144a3106f: Waiting\n",
      "3967b64f52c8: Waiting\n",
      "1dcb04b911f2: Waiting\n",
      "314288c02398: Waiting\n",
      "36043ad8bfd8: Waiting\n",
      "41f9128a0929: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "135516ebbd77: Waiting\n",
      "6b8f1255faed: Waiting\n",
      "088f0f6c3d73: Waiting\n",
      "a2d839c63b4f: Waiting\n",
      "2bec2fd5c215: Waiting\n",
      "3534c2ae025e: Waiting\n",
      "15314207c318: Layer already exists\n",
      "03ef946f32ed: Layer already exists\n",
      "2bec2fd5c215: Layer already exists\n",
      "3534c2ae025e: Layer already exists\n",
      "135516ebbd77: Layer already exists\n",
      "6b8f1255faed: Layer already exists\n",
      "088f0f6c3d73: Layer already exists\n",
      "a2d839c63b4f: Layer already exists\n",
      "f1a50167c61e: Layer already exists\n",
      "db5abdf1ca8e: Layer already exists\n",
      "36febfb24590: Layer already exists\n",
      "efc3379ca486: Layer already exists\n",
      "f964a6d1bdf4: Layer already exists\n",
      "ac196fc8aded: Layer already exists\n",
      "40c691851aa3: Layer already exists\n",
      "a67d17b18bae: Pushed\n",
      "b547475759de: Layer already exists\n",
      "4772fa125f82: Layer already exists\n",
      "18041e4ed604: Pushed\n",
      "d1a7a58f91b9: Layer already exists\n",
      "2acdee852ef3: Layer already exists\n",
      "f6ad8b3b20da: Layer already exists\n",
      "ce71aba18ddd: Layer already exists\n",
      "ec7289967c61: Layer already exists\n",
      "edf4033d0095: Layer already exists\n",
      "7a21321c0b24: Pushed\n",
      "d2e3bf405be8: Layer already exists\n",
      "1ac28deff3c9: Layer already exists\n",
      "8151b6d8bf06: Layer already exists\n",
      "9290a9cc6a8e: Layer already exists\n",
      "11d386c39bde: Layer already exists\n",
      "18efc6c265a6: Layer already exists\n",
      "7a6612c0bb83: Layer already exists\n",
      "05bee8393555: Layer already exists\n",
      "885ff7f3caf7: Layer already exists\n",
      "757f009697c5: Layer already exists\n",
      "68b9b6eb9fad: Layer already exists\n",
      "94e4a30378e5: Layer already exists\n",
      "17989ebae4aa: Layer already exists\n",
      "b4fb898f5841: Layer already exists\n",
      "6e68adc289b2: Layer already exists\n",
      "8144b9447a97: Layer already exists\n",
      "662144a3106f: Layer already exists\n",
      "e94bd04067f0: Layer already exists\n",
      "3967b64f52c8: Layer already exists\n",
      "314288c02398: Layer already exists\n",
      "1dcb04b911f2: Layer already exists\n",
      "41f9128a0929: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "36043ad8bfd8: Layer already exists\n",
      "train: digest: sha256:88c4e278eb3e6185c7388f1657c396dbbd886cc020bfc8c0edee645c8507d4b1 size: 10834\n",
      "Done pushing 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:train\n",
      "Training image: 869814743361.dkr.ecr.us-east-1.amazonaws.com/xview3-training:train\n"
     ]
    }
   ],
   "source": [
    "training_image_uri = build_and_push_docker_image(training_main_name, \n",
    "                                                 dockerfile=str(train_dockerfile),\n",
    "                                                 base_image=base_image_uri)\n",
    "print(f'Training image: {training_image_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_uri = f'{account}.dkr.ecr.{region}.amazonaws.com/xview3-training:train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='/opt/ml/model/FRCNN/auto'\n",
    "shoreline_dir  = '/opt/ml/input/data/shoreline/'\n",
    "\n",
    "metrics = [\n",
    "    {\"Name\": \"training:loss\", \"Regex\": \"total_loss: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:loss_box_reg\", \"Regex\": \"loss_box_reg: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:loss_rpn_loc\", \"Regex\": \"loss_rpn_loc: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:loss_length_reg\", \"Regex\": \"loss_length_reg: ([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"training:lr\", \"Regex\": \"lr: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"training:dataloader_time\", \"Regex\": \"data_time: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"training:time\", \"Regex\": \"time: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation:aggregate\", \"Regex\": \"aggregate=([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"validation:loc_fscore\", \"Regex\": \"loc_fscore=([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"validation:loc_fscore_shore\", \"Regex\": \"loc_fscore_shore=([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"validation:vessel_fscore\", \"Regex\": \"vessel_fscore=([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"validation:fishing_fscore\", \"Regex\": \"fishing_fscore=([0-9\\\\.]+)\",},\n",
    "    {\"Name\": \"validation:length_acc\", \"Regex\": \"length_acc=([0-9\\\\.]+)\",},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iterations_from_epochs(epochs, bs, num_annotations, max_evals, warmup_prop, num_gpus=1):\n",
    "    iter_max = int(num_annotations / (num_gpus * bs) * epochs)\n",
    "    eval_period = iter_max//max_evals\n",
    "    iter_warmup = int(iter_max * warmup_prop)\n",
    "    \n",
    "    return iter_max, eval_period, iter_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(order=True)\n",
    "class Instances:\n",
    "    name: str\n",
    "    num_gpus: int = 1\n",
    "    instance_limit: int = 1\n",
    "    num_workers: int = 4\n",
    "    batch_size: int = 12\n",
    "    volume: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_members = [Instances('local_gpu', num_gpus=4),\n",
    "                    Instances('ml.p3.2xlarge'), \n",
    "                    Instances('ml.p3.8xlarge', 4, 4, 16), \n",
    "                    Instances('ml.p3.16xlarge', 8, 2, 32),\n",
    "                    Instances('ml.p3dn.24xlarge', 8, num_workers=48, batch_size=24, volume=1800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ANNOTS = {'tiny': 1679, \n",
    "              'train': 54360}\n",
    "\n",
    "if USE_CHIPPED:\n",
    "    NUM_ANNOTS['tiny'] = 1907\n",
    "    NUM_ANNOTS['train'] = 62766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instances(name='ml.p3.16xlarge', num_gpus=8, instance_limit=2, num_workers=32, batch_size=12, volume=2048)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = instance_members[-2]\n",
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54360 10872 10872 5436\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "num_annotations = NUM_ANNOTS['tiny'] if USE_TINY else NUM_ANNOTS['train']\n",
    "bs = instance.batch_size\n",
    "#num_gpus = 1 #instance.num_gpus\n",
    "max_evals = 5\n",
    "max_checkpoints = max_evals * 2\n",
    "warmup_prop = 0.2\n",
    "\n",
    "max_iter, eval_period, warmup_iter = compute_iterations_from_epochs(epochs, bs, num_annotations, num_gpus, max_evals, warmup_prop)\n",
    "checkpoint_period = eval_period // 2\n",
    "print(max_iter, eval_period, warmup_iter, checkpoint_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "mode = \"tiny\" if USE_TINY else \"trainval\"\n",
    "imagery_s3_uri = f's3://xview3-blog/data/processing/202207250702/imagery/hdf5/{mode}/'\n",
    "\n",
    "if USE_CHIPPED:\n",
    "    imagery_s3_uri = f's3://xview3-blog/data/processing/202207250702/imagery/chipped-scenes/{mode}/xview3_chipped_2560x2560_{mode.replace(\"val\", \"\")}/'\n",
    "    val_imagery_s3_uri = f's3://xview3-blog/data/processing/202207250702/imagery/hdf5/{mode}/'\n",
    "    s3_channel_valid_imagery = TrainingInput(val_imagery_s3_uri, \n",
    "                                   distribution='FullyReplicated', \n",
    "                                   s3_data_type='S3Prefix',\n",
    "                                   input_mode='FastFile')\n",
    "    \n",
    "shoreline_s3_uri = 's3://xview3-blog/data/shoreline/trainval/'\n",
    "datasets_s3_uri = 's3://xview3-blog/data/processing/202207250702/detectron2_dataset/'\n",
    "\n",
    "s3_channel_imagery = TrainingInput(imagery_s3_uri, \n",
    "                                   distribution='FullyReplicated', \n",
    "                                   s3_data_type='S3Prefix',\n",
    "                                   input_mode='FastFile')\n",
    "s3_channel_shoreline = TrainingInput(shoreline_s3_uri, \n",
    "                                     distribution='FullyReplicated', \n",
    "                                     s3_data_type='S3Prefix', \n",
    "                                     input_mode='FastFile')\n",
    "s3_channel_datasets = TrainingInput(datasets_s3_uri, \n",
    "                                    distribution='FullyReplicated', \n",
    "                                    s3_data_type='S3Prefix',\n",
    "                                    input_mode='FastFile')\n",
    "\n",
    "train_inputs = {'imagery': s3_channel_imagery, \n",
    "                'shoreline': s3_channel_shoreline, \n",
    "                'datasets': s3_channel_datasets}\n",
    "if USE_CHIPPED:\n",
    "    train_inputs['valid_imagery'] = s3_channel_valid_imagery\n",
    "\n",
    "# Use EFS if local\n",
    "if LOCAL:\n",
    "    train_inputs['imagery'] = f'file:////home/ec2-user/SageMaker/xview3-blog/data/imagery/hdf5/tiny/'\n",
    "    train_inputs['shoreline'] = 'file:///home/ec2-user/SageMaker/xview3-blog/data/shoreline/trainval/'\n",
    "    train_inputs['datasets'] = 'file:///home/ec2-user/SageMaker/xview3-blog/data/detectron2_datasets/new/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'frcnn_X101_32x8d_FPN_full.yaml'#'frcnn_R101_FPN_full.yaml'#'frcnn_R101_FPN_full_VH3.yaml' \n",
    "if USE_CHIPPED:\n",
    "    config_file = 'frcnn_R101_FPN_chipped_histeq.yaml'\n",
    "\n",
    "config_params = [f'OUTPUT_DIR {output_dir}',\n",
    "                 f'TEST.INPUT.SHORELINE_DIR {shoreline_dir}',\n",
    "                 f'INPUT.DATA.SHORELINE_DIR {shoreline_dir}',\n",
    "                 f\"SOLVER.IMS_PER_BATCH {bs}\",\n",
    "                 f\"TEST.EVAL_PERIOD {eval_period}\",\n",
    "                 f\"SOLVER.WARMUP_ITERS {warmup_iter}\",\n",
    "                 f\"SOLVER.MAX_ITER {max_iter}\",\n",
    "                 f\"SOLVER.CHECKPOINT_PERIOD {checkpoint_period}\",\n",
    "                 f\"DATALOADER.NUM_WORKERS {instance.num_workers}\",\n",
    "                 \"SOLVER.LR_SCHEDULER_NAME WarmupCosineLR\",\n",
    "                 \"SOLVER.BASE_LR 0.005\",\n",
    "                ]\n",
    "\n",
    "training_job_hp = {'config-file': f'/opt/ml/code/configs/{config_file}',\n",
    "                   'imagery-dir': '/opt/ml/input/data/imagery',\n",
    "                   'd2-dataset-dir': '/opt/ml/input/data/datasets',\n",
    "                   'zopts': ' '.join(config_params)}\n",
    "\n",
    "if USE_CHIPPED:\n",
    "    training_job_hp['valid-imagery-dir'] = '/opt/ml/input/data/valid_imagery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT_DIR /opt/ml/output/FRCNN/auto',\n",
       " 'TEST.INPUT.SHORELINE_DIR /opt/ml/input/data/shoreline/',\n",
       " 'INPUT.DATA.SHORELINE_DIR /opt/ml/input/data/shoreline/',\n",
       " 'SOLVER.IMS_PER_BATCH 6',\n",
       " 'TEST.EVAL_PERIOD 10872',\n",
       " 'SOLVER.WARMUP_ITERS 10872',\n",
       " 'SOLVER.MAX_ITER 54360',\n",
       " 'SOLVER.CHECKPOINT_PERIOD 5436',\n",
       " 'DATALOADER.NUM_WORKERS 32',\n",
       " 'SOLVER.LR_SCHEDULER_NAME WarmupCosineLR',\n",
       " 'SOLVER.BASE_LR 0.005']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#base_job_name = f\"xview3-{'chipped' if USE_CHIPPED else 'full'}-{'tiny' if USE_TINY else 'trainval'}\"\n",
    "base_job_name = f\"xview3-{config_file.split('.')[0].replace('_', '-')}\"\n",
    "\n",
    "training_instance = instance.name\n",
    "num_instances = 1\n",
    "training_session = sagemaker_session\n",
    "\n",
    "\n",
    "if training_instance.startswith(\"local\"):\n",
    "    training_session = sagemaker.LocalSession()\n",
    "    training_session.config = {\"local\": {\"local_code\": True}}\n",
    "    LOCAL = True\n",
    "\n",
    "d2_estimator = Estimator(image_uri=training_image_uri,\n",
    "                         role=role, \n",
    "                         sagemaker_session=training_session, \n",
    "                         instance_count=num_instances, \n",
    "                         instance_type=training_instance, \n",
    "                         volume_size=instance.volume,\n",
    "                         metric_definitions=metrics, \n",
    "                         hyperparameters=training_job_hp,\n",
    "                         base_job_name=base_job_name, \n",
    "                         max_retry_attempts=30, \n",
    "                         max_run=432000,\n",
    "                         checkpoint_local_path=None if LOCAL else '/opt/ml/checkpoints/' ,\n",
    "                         checkpoint_s3_uri=None if LOCAL else 's3://xview3-blog-sagemaker/checkpoints/',\n",
    "                         disable_profiler=True,\n",
    "                         debugger_hook_config=False,\n",
    "                        tags=tags)\n",
    "\n",
    "d2_estimator.fit(inputs=train_inputs, \n",
    "                 wait=True if USE_TINY else False, \n",
    "                 logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`/tmp/tmp7dix_o_f/algo-1-2i620`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66320"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('xview3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e86a4bdce09334967ec02703ecb4f73f52314f4bf74047e8184a512364b7029"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
